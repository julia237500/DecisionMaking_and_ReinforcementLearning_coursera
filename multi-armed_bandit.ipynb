{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multi-arm bandits\n",
    "Multi-arm bandits are a class of problems in which fixed resources must be allocated to different decisions to maximize a return value. Consider a casino with 10 different slot machines to choose from. Assume that each slot machine has an unknown and potentially different mean reward and that you get a random reward (positive or negative) when you play it.\n",
    "\n",
    "To maximize the return in this game, you need to explore all machines to get a sense of reward values. But you do not want to greedily stick to a machine if it returns a high reward once. In other words, you need to explore and gain confidence in the average reward for each slot, which can be achieved using an epsilon-greedy selection strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# required imports\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "%matplotlib inline\n",
    "\n",
    "STEP_SIZE = 0.1\n",
    "K = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy_selection(epsilon, Q):\n",
    "    if np.random.rand() < epsilon:\n",
    "        # Exploration: choose a random action\n",
    "        return np.random.randint(len(Q))\n",
    "    \n",
    "    # Exploitation: choose the action with the highest Q-value\n",
    "    max_value = np.max(Q)\n",
    "    max_indices = np.where(Q == max_value)[0]  # get all indices with max Q\n",
    "    return np.random.choice(max_indices)       # break ties randomly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check\n",
    "rewards = np.array([3, 2, 1])\n",
    "# pure greedy\n",
    "for _ in range(100):\n",
    "    assert epsilon_greedy_selection(0, rewards) == 0\n",
    "    \n",
    "# pure greedy with more than one max reward.\n",
    "rewards = np.array([3, 3, 1, 2])\n",
    "for _ in range(100):\n",
    "    index = epsilon_greedy_selection(0, rewards)\n",
    "    assert index == 0 or index == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# allow some stochasiticity\n",
    "epsilon = 0.1\n",
    "indices = [epsilon_greedy_selection(epsilon, rewards) for _ in range(1000)]\n",
    "obtained_rewards = [rewards[i] for i in indices]\n",
    "obtained_rewards = np.array(obtained_rewards)\n",
    "mean_reward = np.mean(obtained_rewards)\n",
    "assert 2.5 <= mean_reward <= 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have implemented a helper class for you that simulates playing slot machines 1000 times for different values of epsilon. Note that  ðœ–=0\n",
    "  would correspond to a pure greedy strategy while  ðœ–=1\n",
    "  would correspond to a pure random strategy.\n",
    "\n",
    "In your implementation of epsilon_greedy_selection, you should be able to observe that using  ðœ–=0.01\n",
    "  gives a better reward in the long run than using  ðœ–=0\n",
    " ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SlotMachine:\n",
    "    def __init__(self, eps):\n",
    "        self.epsilon = eps\n",
    "        \n",
    "    def reset(self):\n",
    "        # real reward for each action\n",
    "        self.q_true = np.random.randn(K)\n",
    "\n",
    "        # estimation for each action\n",
    "        self.q_estimation = np.zeros(K)\n",
    "        \n",
    "        self.best_action = np.argmax(self.q_true)\n",
    "   \n",
    "    # take an action, update estimation for this action\n",
    "    def step(self, action):\n",
    "        # generate the reward under N(real reward, 1)\n",
    "        reward = np.random.randn() + self.q_true[action]\n",
    "\n",
    "        # update estimation with constant step size\n",
    "        self.q_estimation[action] += STEP_SIZE * (reward - self.q_estimation[action])\n",
    "        return reward\n",
    "\n",
    "\n",
    "def simulate(runs, time, slotMachines):\n",
    "    rewards = np.zeros((len(slotMachines), runs, time))\n",
    "    for i, machine in enumerate(slotMachines):\n",
    "        for r in tqdm(range(runs)):\n",
    "            machine.reset()\n",
    "            for t in range(time):\n",
    "                \n",
    "                # use epislon greedy selection\n",
    "                action = epsilon_greedy_selection(machine.epsilon, machine.q_estimation)\n",
    "                \n",
    "                reward = machine.step(action)\n",
    "                rewards[i, r, t] = reward\n",
    "    mean_rewards = rewards.mean(axis=1)\n",
    "    return mean_rewards\n",
    "\n",
    "\n",
    "runs=400\n",
    "time=1000\n",
    "epsilons = [0.0, 0.01]\n",
    "slotMachines = [SlotMachine(eps) for eps in epsilons]\n",
    "rewards_new = simulate(runs, time, slotMachines)\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "for eps, rewards_new in zip(epsilons, rewards_new):\n",
    "    plt.plot(rewards_new, label='$\\epsilon = %.02f$' % (eps))\n",
    "plt.xlabel('steps')\n",
    "plt.ylabel('average reward')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
